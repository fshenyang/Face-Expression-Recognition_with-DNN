{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_val.ipynb",
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyN/R03WvlJSkU+3DS6K4lM6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmswl0707/Face-Expression-Recognition_with-DNN/blob/main/training_val.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMugGtZMVsBE"
      },
      "source": [
        "# 모델 train / test / validation\n",
        "\n",
        "##데이터 다운 및 데이터 로드\n",
        "\n",
        "1.def load_data():\n",
        "- transfotmation = transforms.Compose(tranaforms.ToTensor, transforms.Nomalize) \n",
        " - transforms.Compose = transformation으로 구성 묶기\n",
        " - transforms.ToTensor = 텐서형태로 변환\n",
        " - transforms.Nomalize = 변형 개체 정규화하기\n",
        "\n",
        "- train/test_dataset=dataset.___('data/', train=True/False, transform=transformation, download=True)\n",
        " - train/test = True, False\n",
        " - download = True 로 설정\n",
        "\n",
        "2. def split_train_val_test(train, test, p = train 데이터 비율)\n",
        ": 전체 데이터셋의 80퍼센트는 train, 20퍼센트는 val (기능 설정)\n",
        "- train_set_sum=int/(len(train_dataset.data)*p)\n",
        " - torch.utils.data.random_split(전체 데이터, train 데이터 갯수, 전체 데이터 - train 데이터 갯수)\n",
        " - torch.utils.data.DataLoader(데이터, batch_size=, shuffle=True/False)\n",
        "\n",
        "\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGiD_mGB7jyV"
      },
      "source": [
        "\n",
        "\n",
        "# mnist 데이터 다운받고 불러오기\n",
        "def load_mnist():\n",
        "    transformation = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,),(0.3081,))])\n",
        "\n",
        "    train_dataset=datasets.MNIST('data/', train=True, transform=transformation, download=True)\n",
        "    test_dataset=datasets.MNIST('data/', train=False, transform=transformation, download=True)\n",
        "\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "#불러온 데이터 로드하기\n",
        "#로드 할때 train 데이터에 20%는 validation을 위해 나\n",
        "def split_train_val_test(train_dataset, test_dataset, p=0.8):\n",
        "    train_set_num = int(len(train_dataset.data)*p)\n",
        "\n",
        "    train_data, val_data = torch.utils.data.random_split(train_dataset, [train_set_num, len(train_dataset)-train_set_num])\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=True)\n",
        "    test_loader=torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NDi6Tzodzo8"
      },
      "source": [
        "##train 메소드\n",
        "1. def train(epoch, model, train_loader, val_loader)\n",
        "- train_loss, train_acc, global_step = 0.0, 0.0, 0으로 초기화\n",
        "- for e in range(epoch): 정해진 에폭동안 상수 e를 돌림\n",
        " - for batch_idx, (input, target) in enumerate(train_loader):\n",
        " batch_idx, (input, target)을 enumerate로 돌림\n",
        " - global_step\n",
        " - \n",
        " - \n",
        "- optimizer.zero_grad() : gradient descent 직전에 초기화\n",
        "- loss.backward() : 설정한 loss로 부터 역전파를 통해 각 loss에 대한 gradient를 구해준다.\n",
        "- optomozer.step() : 매개변수 갱신\n",
        "- with torch.no_grad(): requires_grad=True, True인 모든 작업을 추적\n",
        " - val_acc, val_loss = test(model, val_loader) 객체 설정\n",
        " - writer.add_scalar('Accuracy/val', val_acc, global_step)\n",
        " - writer.add_scalar('Loss/val', val_loss, global_step)\n",
        " - print()\n",
        "- train_loss, train_acc = 0.0, 0.0으로 초기화\n",
        "- epoch, train_loss, train_acc, val_loss, val_acc이 출력됨\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAtjoX2vdNCz"
      },
      "source": [
        "#train 및 test하는 메소\n",
        "def train(epoch, model, train_loader, val_loader):\n",
        "    #model.train()\n",
        "\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    global_step = 0\n",
        "    for e in range(epoch):\n",
        "        for batch_idx, (input, target) in enumerate(train_loader):\n",
        "            global_step = e * len(train_loader) * batch_size + (batch_idx * batch_size)\n",
        "\n",
        "            input, target = input.cuda(), target.cuda()\n",
        "            input, target = Variable(input), Variable(target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(input)\n",
        "            loss=F.nll_loss(output, target)\n",
        "            preds = output.data.max(dim=1, keepdim=True)[1]\n",
        "            acc=preds.eq(target.data.view_as(preds)).cpu().sum()\n",
        "            train_loss +=F.nll_loss(output, target, reduction='sum').data.item()\n",
        "            train_acc +=acc.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_acc, val_loss = test(model, val_loader)\n",
        "            writer.add_scalar('Loss/val', val_loss, global_step)\n",
        "            writer.add_scalar('Accuracy/val', val_acc, global_step)\n",
        "\n",
        "        loss = train_loss/len(train_loader.dataset)\n",
        "        accuracy = 100.0 * train_acc/len(train_loader.dataset)\n",
        "        writer.add_scalar('Loss/train', loss, global_step)\n",
        "        writer.add_scalar('Accuracy/train', accuracy, global_step)\n",
        "\n",
        "        print('Epoch: {0:4} iter: {1:10} loss {2:3.5f} acc {3:3.5f} \\t v_loss {4:3.5f} v_acc {5:3.5f}'\n",
        "              .format(e+1, global_step, loss, accuracy, val_loss, val_acc))\n",
        "        \n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.03\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_fwKWLVd1Q6"
      },
      "source": [
        "##test 메소드\n",
        "\n",
        "- running_acc, running_loss = 0.0 으로 초기화\n",
        " - \n",
        " - \n",
        " - \n",
        " - \n",
        "- accuracy와 loss에 대한 정의\n",
        " - accuracy = 100.0 * running_acc / len(data_loader.dataset)\n",
        "    loss = running_loss / len(data_loader.dataset)\n",
        "- test_loss, test_acc이 출력됨"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RbWbp9HPdFAy"
      },
      "source": [
        "def test(model, data_loader):\n",
        "    #model.eval()\n",
        "\n",
        "    running_acc = 0.0\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (input, target) in enumerate(data_loader):\n",
        "\n",
        "        input, target = input.cuda(), target.cuda()\n",
        "        input, target = Variable(input), Variable(target)\n",
        "\n",
        "        output = model(input)\n",
        "\n",
        "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
        "        acc = preds.eq(target.data.view_as(preds)).cpu().sum()\n",
        "        running_loss +=F.nll_loss(output, target, reduction='sum').data.item()\n",
        "        running_acc += acc.item()\n",
        "\n",
        "    accuracy = 100.0 * running_acc / len(data_loader.dataset)\n",
        "    loss = running_loss / len(data_loader.dataset)\n",
        "\n",
        "    return accuracy, loss\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RriIdUgmeMPQ"
      },
      "source": [
        "#summary writer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFVyCBjVeUz-"
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "    writer = SummaryWriter()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #hyperparameter\n",
        "    lr = 0.003\n",
        "    epoch = 50\n",
        "    batch_size = 512\n",
        "    train_val_p=0.8\n",
        "\n",
        "    #데이터 셋 로드 및 train, val, test로 데이터 셋 나눔\n",
        "    train_dataset, test_dataset = load_mnist()\n",
        "    train_loader, val_loader, test_loader = split_train_val_test(train_dataset, test_dataset, p=train_val_p)\n",
        "\n",
        "    #모델 불러오고, optimizer 설정\n",
        "    model = Net().to(device)                            # 모델 불러오기\n",
        "    optimizer = optim.SGD(model.parameters(), lr)      #optimizer 설정\n",
        "\n",
        "    #layer 정보 프린트 하기\n",
        "    print(summary(model, input_size=(1, 28, 28)))\n",
        "\n",
        "    train(epoch, model, train_loader, val_loader)   #학습\n",
        "    acc, _ = test(model, test_loader)               #테스트\n",
        "\n",
        "    print('test acc {0:3.5f}' .format(acc))\n",
        "    writer.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}