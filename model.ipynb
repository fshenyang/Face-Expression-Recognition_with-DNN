{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP0g4nVPPz7XoYygeSqCfMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmswl0707/Face-Expression-Recognition_with-DNN/blob/main/ResNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKk6HM7Wk2bw",
        "outputId": "560a8c0c-87d5-4439-cf2e-43022fd84edb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()\n",
        "import platform\n",
        "platform.platform()\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "def conv_block_1(in_dim, out_dim, act_fn, stride=1):\n",
        "    model=nn.Sequential(\n",
        "        nn.Conv2d(in_dim,out_dim, kernerl_size=1, stride=stride),\n",
        "        act_fn, ##???\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def conv_block_3(in_dim, out_dim, act_fn):\n",
        "    model=nn.Sequential(\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),\n",
        "        act_fn,\n",
        "    )\n",
        "    return model \n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    def __init__(self, in_dim, mid_dim, out_dim, act_fn,down=False):\n",
        "        super(BottleNeck, self).__init__()\n",
        "        self.act_fn=act_fn\n",
        "        self.down=down\n",
        "\n",
        "        if self.down:\n",
        "            self.layer=nn.Sequential(\n",
        "                conv_block_1(in_dim, mid_dim, act_fn,2),\n",
        "                conv_block_3(mid_dim, mid_dim, act_fn),\n",
        "                conv_block_1(mid_dim,out_dim, act_fn),\n",
        "            )\n",
        "            self.downsample=nn.Conv2d(in_dim, out_dim,1,2)\n",
        "        else:\n",
        "            self.layer=nn.Sequential(\n",
        "                conv_block_1(in_dim, mid_dim, act_fn),\n",
        "                conv_block_3(mid_dim, mid_dim, act_fn),\n",
        "                conv_block_1(mid_dim, out_dim, act_fn),\n",
        "            )\n",
        "            self.dim_equalizer=nn.Conv2d(in_dim, out_dim, kernel_size=1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            if self.down:\n",
        "                dpwnsample=self.downsample(x)\n",
        "                out=self.layer(x)\n",
        "                out=out+downsample\n",
        "            else:\n",
        "                out=self.layer(x)\n",
        "                if x.size() is not out.size():\n",
        "                    x=self.dim_equalizer(x)\n",
        "                out=out+x\n",
        "            return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, base_dim, num_classes=2):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.act_fn=nn.ReLU()\n",
        "        self.layer_1=nn.Sequential(\n",
        "            nn.Conv2d(3, base_dim,7,2,3),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(3,2,1),\n",
        "        )\n",
        "        self.layer_2=nn.Sequential(\n",
        "            BottleNeck(base_dim, base_dim, base_dim*4, self.act_fn),\n",
        "            BottleNeck(base_dim*4, base_dim, base_dim*4, self.act_fn),\n",
        "            BottleNeck(base_dim*4, base_dim, base_dim*4, self.act_fn, down=True),\n",
        "        )\n",
        "        self.layer_3=nn.Sequential(\n",
        "            BottleNeck(base_dim*4, base_dim*2, base_dim*8, self.act_fn),\n",
        "            BottleNeck(base_dim*8, base_dim*2, base_dim*8, self.act_fn),\n",
        "            BottleNeck(base_dim*8, base_dim*2, base_dim*8, self.act_fn),\n",
        "            BottleNeck(base_dim*8, base_dim*2, base_dim*8, self.act_fn, down=True),\n",
        "        )\n",
        "        self.layer_4=nn.Sequential(\n",
        "            BottleNeck(base_dim*8, base_dim*4, base_dim*16, self.act_fn),\n",
        "            BottleNeck(base_dim*16, base_dim*4, base_dim*16, self.act_fn),\n",
        "            BottleNeck(base_dim*16, base_dim*4, base_dim*16, self.act_fn),\n",
        "            BottleNeck(base_dim*16, base_dim*4, base_dim*16, self.act_fn),\n",
        "            BottleNeck(base_dim*16, base_dim*4, base_dim*16, self.act_fn),\n",
        "            BottleNeck(base_dim*16, base_dim*4, base_dim*16, self.act_fn, down=True),\n",
        "        \n",
        "        self.layer_5=nn.Sequential(\n",
        "            BottleNeck(base_dim*16, base_dim*8, base_dim*32, self.act_fn),\n",
        "            BottleNeck(base_dim*32, base_dim*8, base_dim*32, self.act_fn),\n",
        "            BottleNeck(base_dim*32, base_dim*8, base_dim*32, self.act_fn),\n",
        "        )\n",
        "        self.avgpool=nn.AvgPool2d(7,1)\n",
        "        self.fc_layer=nn.Linear(base_dim*32, num_classes)\n",
        "\n",
        "        def forward(self,x):\n",
        "            out=self.layer_1(x)\n",
        "            out=self.layer_2(out)\n",
        "            out=self.layer_3(out)\n",
        "            out=self.layer_4(out)\n",
        "            out=self.layer_5(out)\n",
        "            out=self.avgpool(out)\n",
        "            out=out.view(batch_size, -1)\n",
        "            out=self.fc_layer(out)\n",
        "            return out\n",
        "\n",
        "def train(epoch, model, data_loader):\n",
        "    print(\"==========training==========\")\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for e in range(epoch):      #에폭 만큼 반복\n",
        "        for batch_idx, (input, target) in enumerate(data_loader):       # 배치 사이즈 만큼 반복\n",
        "            global_step = e * len(data_loader)*batch_size + (batch_idx * batch_size)\n",
        "\n",
        "            input, target = input.cuda(), target.cuda()\n",
        "            input, target = Variable(input, volatile=False), Variable(target)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(input)\n",
        "\n",
        "            loss=F.nll_loss(output, target)\n",
        "            preds = output.data.max(dim=1, keepdim=True)[1]\n",
        "            acc=preds.eq(target.data.view_as(preds)).cpu().sum()\n",
        "            running_loss += loss.item()\n",
        "            running_acc+=acc.item()\n",
        "\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            if batch_idx%100==0:\n",
        "                loss = running_loss/100\n",
        "                accuracy = running_acc/100\n",
        "                writer.add_scalar('loss', loss, global_step)        #tensorboard scalar add\n",
        "                writer.add_scalar('accuracy', accuracy, global_step)\n",
        "\n",
        "                print('Epoch: {:<4} iter: {:>4} loss {} acc {}' .format(e+1, global_step, loss, accuracy))\n",
        "                running_loss = 0.0\n",
        "                running_acc = 0.0\n",
        "\n",
        "def test(model, data_loader):\n",
        "    model.eval()\n",
        "    volatile = True\n",
        "    running_acc = 0.0\n",
        "\n",
        "    for batch_idx, (input, target) in enumerate(data_loader):\n",
        "        global_step = batch_idx * batch_size\n",
        "\n",
        "        input, target = input.cuda(), target.cuda()\n",
        "        input, target = Variable(input, volatile=True), Variable(target)\n",
        "\n",
        "        output = model(input)\n",
        "        preds = output.data.max(dim=1, keepdim=True)[1]\n",
        "        acc = preds.eq(target.data.view_as(preds)).cpu().sum()\n",
        "        running_acc += acc.item()\n",
        "\n",
        "\n",
        "\n",
        "    accuracy = running_acc / len(data_loader)\n",
        "\n",
        "    print('test acc {}'.format(accuracy))\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    writer = SummaryWriter()\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    #hyperparameter\n",
        "    lr = 0.005\n",
        "    epoch = 50\n",
        "    batch_size = 100\n",
        "\n",
        "    train_loader, test_loader = load_mnist(batch_size)        #데이터 셋 로드\n",
        "\n",
        "    model = Net().to(device)                            # 모델 불러오기\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr)      #optimizer 설정\n",
        "\n",
        "    print(summary(model, input_size=(1, 28, 28)))\n",
        "\n",
        "    train(epoch, model, train_loader)\n",
        "    test(model, test_loader)\n",
        "    writer.close()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-efbc020603fe>\"\u001b[0;36m, line \u001b[0;32m90\u001b[0m\n\u001b[0;31m    self.avgpool=nn.AvgPool2d(7,1)\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}
